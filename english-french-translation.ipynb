{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8399195,"sourceType":"datasetVersion","datasetId":4997156}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport keras\nimport os\nfrom pathlib import Path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T10:28:32.318679Z","iopub.execute_input":"2024-06-13T10:28:32.319408Z","iopub.status.idle":"2024-06-13T10:28:32.324413Z","shell.execute_reply.started":"2024-06-13T10:28:32.319375Z","shell.execute_reply":"2024-06-13T10:28:32.323153Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"batch_size = 64  # Batch size for training.\nepochs = 100  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nnum_samples = 10000  # Number of samples to train on.\n# Path to the data txt file on disk.\ndirpath = \"/kaggle/input/english-french-translation-dataset\"\ndata_path = os.path.join(dirpath, \"fra.txt\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:32:40.198473Z","iopub.execute_input":"2024-06-13T10:32:40.198899Z","iopub.status.idle":"2024-06-13T10:32:40.204444Z","shell.execute_reply.started":"2024-06-13T10:32:40.198867Z","shell.execute_reply":"2024-06-13T10:32:40.203411Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Vectorize the data.\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\nwith open(data_path, \"r\", encoding=\"utf-8\") as f:\n    lines = f.read().split(\"\\n\")\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text, _ = line.split(\"\\t\")\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_text = \"\\t\" + target_text + \"\\n\"\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:32:42.570376Z","iopub.execute_input":"2024-06-13T10:32:42.570841Z","iopub.status.idle":"2024-06-13T10:32:43.115258Z","shell.execute_reply.started":"2024-06-13T10:32:42.570799Z","shell.execute_reply":"2024-06-13T10:32:43.114221Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"input_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint(\"Number of samples:\", len(input_texts))\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)\n\ninput_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict([(char, i) for i, char in enumerate(target_characters)])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:33:11.451640Z","iopub.execute_input":"2024-06-13T10:33:11.452048Z","iopub.status.idle":"2024-06-13T10:33:11.462975Z","shell.execute_reply.started":"2024-06-13T10:33:11.452017Z","shell.execute_reply":"2024-06-13T10:33:11.461831Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Number of samples: 10000\nNumber of unique input tokens: 70\nNumber of unique output tokens: 91\nMax sequence length for inputs: 14\nMax sequence length for outputs: 59\n","output_type":"stream"}]},{"cell_type":"code","source":"encoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n    dtype=\"float32\",\n)\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype=\"float32\",\n)\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype=\"float32\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:33:26.657556Z","iopub.execute_input":"2024-06-13T10:33:26.658729Z","iopub.status.idle":"2024-06-13T10:33:26.665252Z","shell.execute_reply.started":"2024-06-13T10:33:26.658690Z","shell.execute_reply":"2024-06-13T10:33:26.663823Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1.0\n    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t, target_token_index[char]] = 1.0\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:33:36.771004Z","iopub.execute_input":"2024-06-13T10:33:36.771506Z","iopub.status.idle":"2024-06-13T10:33:37.411988Z","shell.execute_reply.started":"2024-06-13T10:33:36.771467Z","shell.execute_reply":"2024-06-13T10:33:37.410807Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define an input sequence and process it.\nencoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\nencoder = keras.layers.LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\ndecoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:33:48.129961Z","iopub.execute_input":"2024-06-13T10:33:48.130403Z","iopub.status.idle":"2024-06-13T10:33:48.325525Z","shell.execute_reply.started":"2024-06-13T10:33:48.130369Z","shell.execute_reply":"2024-06-13T10:33:48.324330Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n)\nmodel.fit(\n    [encoder_input_data, decoder_input_data],\n    decoder_target_data,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=0.2,\n)\n# Save model\nmodel.save(\"s2s_model.keras\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:34:00.224050Z","iopub.execute_input":"2024-06-13T10:34:00.224434Z","iopub.status.idle":"2024-06-13T11:23:54.254801Z","shell.execute_reply.started":"2024-06-13T10:34:00.224403Z","shell.execute_reply":"2024-06-13T11:23:54.253307Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 213ms/step - accuracy: 0.7067 - loss: 1.5270 - val_accuracy: 0.7140 - val_loss: 1.0696\nEpoch 2/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 203ms/step - accuracy: 0.7459 - loss: 0.9658 - val_accuracy: 0.7293 - val_loss: 0.9625\nEpoch 3/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 205ms/step - accuracy: 0.7625 - loss: 0.8699 - val_accuracy: 0.7462 - val_loss: 0.8933\nEpoch 4/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.7848 - loss: 0.7720 - val_accuracy: 0.7734 - val_loss: 0.7820\nEpoch 5/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 203ms/step - accuracy: 0.8028 - loss: 0.6853 - val_accuracy: 0.7854 - val_loss: 0.7282\nEpoch 6/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 215ms/step - accuracy: 0.8145 - loss: 0.6369 - val_accuracy: 0.7959 - val_loss: 0.7068\nEpoch 7/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 210ms/step - accuracy: 0.8222 - loss: 0.6074 - val_accuracy: 0.8099 - val_loss: 0.6572\nEpoch 8/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 206ms/step - accuracy: 0.8303 - loss: 0.5796 - val_accuracy: 0.8050 - val_loss: 0.6527\nEpoch 9/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.8336 - loss: 0.5657 - val_accuracy: 0.8164 - val_loss: 0.6261\nEpoch 10/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 230ms/step - accuracy: 0.8399 - loss: 0.5432 - val_accuracy: 0.8260 - val_loss: 0.5995\nEpoch 11/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 232ms/step - accuracy: 0.8452 - loss: 0.5265 - val_accuracy: 0.8302 - val_loss: 0.5876\nEpoch 12/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 239ms/step - accuracy: 0.8501 - loss: 0.5127 - val_accuracy: 0.8311 - val_loss: 0.5762\nEpoch 13/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 222ms/step - accuracy: 0.8521 - loss: 0.5042 - val_accuracy: 0.8368 - val_loss: 0.5618\nEpoch 14/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 205ms/step - accuracy: 0.8579 - loss: 0.4843 - val_accuracy: 0.8370 - val_loss: 0.5546\nEpoch 15/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 205ms/step - accuracy: 0.8602 - loss: 0.4760 - val_accuracy: 0.8367 - val_loss: 0.5611\nEpoch 16/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 210ms/step - accuracy: 0.8632 - loss: 0.4635 - val_accuracy: 0.8436 - val_loss: 0.5320\nEpoch 17/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.8665 - loss: 0.4515 - val_accuracy: 0.8431 - val_loss: 0.5326\nEpoch 18/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.8689 - loss: 0.4440 - val_accuracy: 0.8476 - val_loss: 0.5211\nEpoch 19/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.8710 - loss: 0.4335 - val_accuracy: 0.8493 - val_loss: 0.5131\nEpoch 20/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 204ms/step - accuracy: 0.8741 - loss: 0.4230 - val_accuracy: 0.8508 - val_loss: 0.5043\nEpoch 21/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 207ms/step - accuracy: 0.8761 - loss: 0.4171 - val_accuracy: 0.8531 - val_loss: 0.4974\nEpoch 22/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 207ms/step - accuracy: 0.8778 - loss: 0.4118 - val_accuracy: 0.8538 - val_loss: 0.4961\nEpoch 23/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 207ms/step - accuracy: 0.8800 - loss: 0.4020 - val_accuracy: 0.8566 - val_loss: 0.4910\nEpoch 24/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 213ms/step - accuracy: 0.8829 - loss: 0.3954 - val_accuracy: 0.8574 - val_loss: 0.4869\nEpoch 25/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 215ms/step - accuracy: 0.8850 - loss: 0.3862 - val_accuracy: 0.8596 - val_loss: 0.4810\nEpoch 26/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 212ms/step - accuracy: 0.8864 - loss: 0.3815 - val_accuracy: 0.8598 - val_loss: 0.4750\nEpoch 27/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 211ms/step - accuracy: 0.8891 - loss: 0.3721 - val_accuracy: 0.8605 - val_loss: 0.4730\nEpoch 28/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 207ms/step - accuracy: 0.8892 - loss: 0.3700 - val_accuracy: 0.8626 - val_loss: 0.4691\nEpoch 29/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 205ms/step - accuracy: 0.8920 - loss: 0.3612 - val_accuracy: 0.8630 - val_loss: 0.4667\nEpoch 30/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.8934 - loss: 0.3581 - val_accuracy: 0.8646 - val_loss: 0.4622\nEpoch 31/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 203ms/step - accuracy: 0.8952 - loss: 0.3531 - val_accuracy: 0.8641 - val_loss: 0.4636\nEpoch 32/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 203ms/step - accuracy: 0.8958 - loss: 0.3474 - val_accuracy: 0.8660 - val_loss: 0.4570\nEpoch 33/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 205ms/step - accuracy: 0.8975 - loss: 0.3421 - val_accuracy: 0.8660 - val_loss: 0.4548\nEpoch 34/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 204ms/step - accuracy: 0.8997 - loss: 0.3359 - val_accuracy: 0.8669 - val_loss: 0.4555\nEpoch 35/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 213ms/step - accuracy: 0.9013 - loss: 0.3305 - val_accuracy: 0.8685 - val_loss: 0.4504\nEpoch 36/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.9018 - loss: 0.3292 - val_accuracy: 0.8679 - val_loss: 0.4520\nEpoch 37/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.9034 - loss: 0.3250 - val_accuracy: 0.8686 - val_loss: 0.4490\nEpoch 38/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 205ms/step - accuracy: 0.9047 - loss: 0.3199 - val_accuracy: 0.8692 - val_loss: 0.4499\nEpoch 39/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 203ms/step - accuracy: 0.9051 - loss: 0.3175 - val_accuracy: 0.8713 - val_loss: 0.4447\nEpoch 40/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 209ms/step - accuracy: 0.9072 - loss: 0.3106 - val_accuracy: 0.8710 - val_loss: 0.4439\nEpoch 41/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.9090 - loss: 0.3045 - val_accuracy: 0.8705 - val_loss: 0.4466\nEpoch 42/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 218ms/step - accuracy: 0.9113 - loss: 0.2988 - val_accuracy: 0.8723 - val_loss: 0.4420\nEpoch 43/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 220ms/step - accuracy: 0.9109 - loss: 0.2986 - val_accuracy: 0.8714 - val_loss: 0.4449\nEpoch 44/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 215ms/step - accuracy: 0.9124 - loss: 0.2930 - val_accuracy: 0.8737 - val_loss: 0.4394\nEpoch 45/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 207ms/step - accuracy: 0.9135 - loss: 0.2884 - val_accuracy: 0.8728 - val_loss: 0.4401\nEpoch 46/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 215ms/step - accuracy: 0.9142 - loss: 0.2866 - val_accuracy: 0.8741 - val_loss: 0.4405\nEpoch 47/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 207ms/step - accuracy: 0.9159 - loss: 0.2826 - val_accuracy: 0.8740 - val_loss: 0.4411\nEpoch 48/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 216ms/step - accuracy: 0.9169 - loss: 0.2775 - val_accuracy: 0.8738 - val_loss: 0.4418\nEpoch 49/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 213ms/step - accuracy: 0.9169 - loss: 0.2762 - val_accuracy: 0.8747 - val_loss: 0.4390\nEpoch 50/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 211ms/step - accuracy: 0.9193 - loss: 0.2683 - val_accuracy: 0.8743 - val_loss: 0.4418\nEpoch 51/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.9208 - loss: 0.2640 - val_accuracy: 0.8749 - val_loss: 0.4396\nEpoch 52/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.9216 - loss: 0.2627 - val_accuracy: 0.8754 - val_loss: 0.4407\nEpoch 53/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - accuracy: 0.9224 - loss: 0.2597 - val_accuracy: 0.8754 - val_loss: 0.4445\nEpoch 54/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 211ms/step - accuracy: 0.9244 - loss: 0.2524 - val_accuracy: 0.8743 - val_loss: 0.4437\nEpoch 55/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 209ms/step - accuracy: 0.9242 - loss: 0.2511 - val_accuracy: 0.8747 - val_loss: 0.4456\nEpoch 56/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 205ms/step - accuracy: 0.9266 - loss: 0.2455 - val_accuracy: 0.8753 - val_loss: 0.4479\nEpoch 57/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 207ms/step - accuracy: 0.9266 - loss: 0.2436 - val_accuracy: 0.8756 - val_loss: 0.4435\nEpoch 58/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 213ms/step - accuracy: 0.9275 - loss: 0.2410 - val_accuracy: 0.8758 - val_loss: 0.4439\nEpoch 59/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 216ms/step - accuracy: 0.9296 - loss: 0.2358 - val_accuracy: 0.8760 - val_loss: 0.4451\nEpoch 60/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 213ms/step - accuracy: 0.9300 - loss: 0.2346 - val_accuracy: 0.8753 - val_loss: 0.4480\nEpoch 61/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 210ms/step - accuracy: 0.9298 - loss: 0.2333 - val_accuracy: 0.8762 - val_loss: 0.4463\nEpoch 62/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 209ms/step - accuracy: 0.9326 - loss: 0.2259 - val_accuracy: 0.8759 - val_loss: 0.4472\nEpoch 63/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.9328 - loss: 0.2246 - val_accuracy: 0.8762 - val_loss: 0.4468\nEpoch 64/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.9334 - loss: 0.2210 - val_accuracy: 0.8760 - val_loss: 0.4534\nEpoch 65/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.9354 - loss: 0.2160 - val_accuracy: 0.8761 - val_loss: 0.4529\nEpoch 66/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 205ms/step - accuracy: 0.9362 - loss: 0.2128 - val_accuracy: 0.8767 - val_loss: 0.4528\nEpoch 67/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.9364 - loss: 0.2110 - val_accuracy: 0.8765 - val_loss: 0.4554\nEpoch 68/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.9379 - loss: 0.2077 - val_accuracy: 0.8756 - val_loss: 0.4596\nEpoch 69/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 207ms/step - accuracy: 0.9386 - loss: 0.2050 - val_accuracy: 0.8751 - val_loss: 0.4608\nEpoch 70/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 221ms/step - accuracy: 0.9392 - loss: 0.2024 - val_accuracy: 0.8762 - val_loss: 0.4578\nEpoch 71/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 214ms/step - accuracy: 0.9395 - loss: 0.2018 - val_accuracy: 0.8759 - val_loss: 0.4616\nEpoch 72/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 209ms/step - accuracy: 0.9413 - loss: 0.1958 - val_accuracy: 0.8761 - val_loss: 0.4641\nEpoch 73/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 216ms/step - accuracy: 0.9415 - loss: 0.1942 - val_accuracy: 0.8760 - val_loss: 0.4657\nEpoch 74/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 211ms/step - accuracy: 0.9424 - loss: 0.1923 - val_accuracy: 0.8753 - val_loss: 0.4672\nEpoch 75/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 218ms/step - accuracy: 0.9435 - loss: 0.1879 - val_accuracy: 0.8763 - val_loss: 0.4706\nEpoch 76/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 236ms/step - accuracy: 0.9447 - loss: 0.1848 - val_accuracy: 0.8744 - val_loss: 0.4715\nEpoch 77/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 227ms/step - accuracy: 0.9447 - loss: 0.1850 - val_accuracy: 0.8761 - val_loss: 0.4725\nEpoch 78/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 211ms/step - accuracy: 0.9450 - loss: 0.1823 - val_accuracy: 0.8747 - val_loss: 0.4781\nEpoch 79/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 212ms/step - accuracy: 0.9461 - loss: 0.1790 - val_accuracy: 0.8747 - val_loss: 0.4806\nEpoch 80/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 231ms/step - accuracy: 0.9470 - loss: 0.1775 - val_accuracy: 0.8757 - val_loss: 0.4777\nEpoch 81/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 233ms/step - accuracy: 0.9475 - loss: 0.1750 - val_accuracy: 0.8756 - val_loss: 0.4796\nEpoch 82/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 247ms/step - accuracy: 0.9485 - loss: 0.1711 - val_accuracy: 0.8747 - val_loss: 0.4856\nEpoch 83/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 230ms/step - accuracy: 0.9485 - loss: 0.1695 - val_accuracy: 0.8744 - val_loss: 0.4881\nEpoch 84/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 214ms/step - accuracy: 0.9500 - loss: 0.1666 - val_accuracy: 0.8754 - val_loss: 0.4866\nEpoch 85/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 212ms/step - accuracy: 0.9499 - loss: 0.1661 - val_accuracy: 0.8754 - val_loss: 0.4914\nEpoch 86/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 239ms/step - accuracy: 0.9506 - loss: 0.1634 - val_accuracy: 0.8753 - val_loss: 0.4953\nEpoch 87/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 231ms/step - accuracy: 0.9517 - loss: 0.1608 - val_accuracy: 0.8747 - val_loss: 0.4971\nEpoch 88/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 220ms/step - accuracy: 0.9519 - loss: 0.1595 - val_accuracy: 0.8758 - val_loss: 0.4980\nEpoch 89/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 226ms/step - accuracy: 0.9522 - loss: 0.1573 - val_accuracy: 0.8742 - val_loss: 0.5009\nEpoch 90/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 244ms/step - accuracy: 0.9530 - loss: 0.1549 - val_accuracy: 0.8743 - val_loss: 0.5024\nEpoch 91/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 238ms/step - accuracy: 0.9539 - loss: 0.1528 - val_accuracy: 0.8752 - val_loss: 0.5053\nEpoch 92/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 225ms/step - accuracy: 0.9543 - loss: 0.1511 - val_accuracy: 0.8749 - val_loss: 0.5055\nEpoch 93/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 213ms/step - accuracy: 0.9550 - loss: 0.1492 - val_accuracy: 0.8744 - val_loss: 0.5092\nEpoch 94/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 209ms/step - accuracy: 0.9550 - loss: 0.1494 - val_accuracy: 0.8742 - val_loss: 0.5116\nEpoch 95/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - accuracy: 0.9558 - loss: 0.1467 - val_accuracy: 0.8748 - val_loss: 0.5147\nEpoch 96/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 209ms/step - accuracy: 0.9569 - loss: 0.1439 - val_accuracy: 0.8739 - val_loss: 0.5192\nEpoch 97/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.9569 - loss: 0.1419 - val_accuracy: 0.8743 - val_loss: 0.5205\nEpoch 98/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 208ms/step - accuracy: 0.9579 - loss: 0.1393 - val_accuracy: 0.8749 - val_loss: 0.5211\nEpoch 99/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - accuracy: 0.9579 - loss: 0.1387 - val_accuracy: 0.8745 - val_loss: 0.5242\nEpoch 100/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 209ms/step - accuracy: 0.9592 - loss: 0.1352 - val_accuracy: 0.8747 - val_loss: 0.5271\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:33:05.311050Z","iopub.execute_input":"2024-06-13T11:33:05.311434Z","iopub.status.idle":"2024-06-13T11:33:05.340934Z","shell.execute_reply.started":"2024-06-13T11:33:05.311404Z","shell.execute_reply":"2024-06-13T11:33:05.339673Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m91\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m334,848\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m356,352\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m91\u001b[0m)  │     \u001b[38;5;34m23,387\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">91</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">334,848</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">356,352</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">91</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,387</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,429,176\u001b[0m (5.45 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,429,176</span> (5.45 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m714,587\u001b[0m (2.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">714,587</span> (2.73 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m714,589\u001b[0m (2.73 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">714,589</span> (2.73 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Define sampling models\n# Restore the model and construct the encoder and decoder.\nmodel = keras.models.load_model(\"s2s_model.keras\")\n\nencoder_inputs = model.input[0]  # input_1\nencoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\nencoder_states = [state_h_enc, state_c_enc]\nencoder_model = keras.Model(encoder_inputs, encoder_states)\n\ndecoder_inputs = model.input[1]  # input_2\ndecoder_state_input_h = keras.Input(shape=(latent_dim,))\ndecoder_state_input_c = keras.Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_lstm = model.layers[3]\ndecoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs\n)\ndecoder_states = [state_h_dec, state_c_dec]\ndecoder_dense = model.layers[4]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = keras.Model(\n    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n)\n\n# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq, verbose=0)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = \"\"\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value, verbose=0\n        )\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.0\n\n        # Update states\n        states_value = [h, c]\n    return decoded_sentence\n# You can now generate decoded sentences as such:\n\nfor seq_index in range(20):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index : seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print(\"-\")\n    print(\"Input sentence:\", input_texts[seq_index])\n    print(\"Decoded sentence:\", decoded_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:31:04.397349Z","iopub.execute_input":"2024-06-13T11:31:04.397711Z","iopub.status.idle":"2024-06-13T11:31:18.645773Z","shell.execute_reply.started":"2024-06-13T11:31:04.397683Z","shell.execute_reply":"2024-06-13T11:31:18.644510Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"-\nInput sentence: Go.\nDecoded sentence: Coure.\n\n-\nInput sentence: Go.\nDecoded sentence: Coure.\n\n-\nInput sentence: Go.\nDecoded sentence: Coure.\n\n-\nInput sentence: Go.\nDecoded sentence: Coure.\n\n-\nInput sentence: Hi.\nDecoded sentence: Salut !\n\n-\nInput sentence: Hi.\nDecoded sentence: Salut !\n\n-\nInput sentence: Run!\nDecoded sentence: File !\n\n-\nInput sentence: Run!\nDecoded sentence: File !\n\n-\nInput sentence: Run!\nDecoded sentence: File !\n\n-\nInput sentence: Run!\nDecoded sentence: File !\n\n-\nInput sentence: Run!\nDecoded sentence: File !\n\n-\nInput sentence: Run!\nDecoded sentence: File !\n\n-\nInput sentence: Run!\nDecoded sentence: File !\n\n-\nInput sentence: Run!\nDecoded sentence: File !\n\n-\nInput sentence: Run.\nDecoded sentence: File !\n\n-\nInput sentence: Run.\nDecoded sentence: File !\n\n-\nInput sentence: Run.\nDecoded sentence: File !\n\n-\nInput sentence: Run.\nDecoded sentence: File !\n\n-\nInput sentence: Run.\nDecoded sentence: File !\n\n-\nInput sentence: Run.\nDecoded sentence: File !\n\n","output_type":"stream"}]}]}